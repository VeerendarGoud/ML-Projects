{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxWgy1jr3JeAi+u7RwwaKR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VeerendarGoud/ML-Projects/blob/master/Pyspark_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oapIQMX1TTk7"
      },
      "outputs": [],
      "source": [
        "# from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "6VABSfAUT46K"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install java\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "tsBG_lyiT43i"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
        "os.environ['PATH'] = os.environ['JAVA_HOME'] + '/bin:' + os.environ['PATH']"
      ],
      "metadata": {
        "id": "WqaSoQQkUMKH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54DQ46fwULWd",
        "outputId": "84712f3e-8298-4e4d-e8bd-e37fb93f9357"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"1.8.0_382\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_382-8u382-ga-1~22.04.1-b05)\n",
            "OpenJDK 64-Bit Server VM (build 25.382-b05, mixed mode)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.3.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnO5XYKqULT4",
        "outputId": "8ccdd07d-f6a5-4384-f6f0-0c224275b0b1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.3.0\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5 (from pyspark==3.3.0)\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764004 sha256=8fb78258e99adb52416b7aa8bd120ad0e007f3d792ac355aeccb75a911c51e26\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/9c/6c/d5200fcf351ffa39cbe09911e99703283624cd037df58070d9\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "ESZSCDQgT41g"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, date\n",
        "import pandas as pd\n",
        "\n",
        "from pyspark.sql import Row"
      ],
      "metadata": {
        "id": "mNeZSxbqTmAg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame([\n",
        "    Row(a=1, b=2, c='string1', d=date(2000,1,1), e=datetime(2000, 1, 1, 12, 0)),\n",
        "    Row(a=2, b=3, c='string2', d=date(2000,1,1), e=datetime(2000, 1, 1, 12, 0)),\n",
        "    Row(a=4, b=5, c='string3', d=date(2000,1,1), e=datetime(2000, 1, 1, 12, 0))\n",
        "])"
      ],
      "metadata": {
        "id": "VFwh-PoQVJ9I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hc7UP4ahVJ6h",
        "outputId": "7e4076df-d2cd-43d6-df81-e58140d78ff2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- a: long (nullable = true)\n",
            " |-- b: long (nullable = true)\n",
            " |-- c: string (nullable = true)\n",
            " |-- d: date (nullable = true)\n",
            " |-- e: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqHmyWb4VJ36",
        "outputId": "ceb567b9-ffdb-46df-ec61-28f80bb8f143"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType([StructField('a', LongType(), True), StructField('b', LongType(), True), StructField('c', StringType(), True), StructField('d', DateType(), True), StructField('e', TimestampType(), True)])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame([\n",
        "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
        "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
        "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
        "], schema='a long, b double, c string, d date, e timestamp')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1CR1JaTVJ1U",
        "outputId": "0f80e297-4771-47e4-ce06-2ed6a1fc1da6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pandas_df = pd.DataFrame({\n",
        "    'a': [1, 2, 3],\n",
        "    'b': [2., 3., 4.],\n",
        "    'c': ['string1', 'string2', 'string3'],\n",
        "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
        "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
        "})\n",
        "df = spark.createDataFrame(pandas_df)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si17T9NTVJwE",
        "outputId": "437c093f-449d-4b6a-cb88-63ce54a096bd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
            "  for column, series in pdf.iteritems():\n",
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
            "  for column, series in pdf.iteritems():\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpcsbTzyVJte",
        "outputId": "0e807fca-0b4f-4ec4-e250-12fad0548c2b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-------+----------+-------------------+\n",
            "|  a|  b|      c|         d|                  e|\n",
            "+---+---+-------+----------+-------------------+\n",
            "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
            "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
            "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
            "+---+---+-------+----------+-------------------+\n",
            "\n",
            "root\n",
            " |-- a: long (nullable = true)\n",
            " |-- b: double (nullable = true)\n",
            " |-- c: string (nullable = true)\n",
            " |-- d: date (nullable = true)\n",
            " |-- e: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Viewing Data"
      ],
      "metadata": {
        "id": "A1weRatrWdTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPBcEJKiVJqU",
        "outputId": "7a272ffd-e0bb-4f60-db66-404afc54da26"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-------+----------+-------------------+\n",
            "|  a|  b|      c|         d|                  e|\n",
            "+---+---+-------+----------+-------------------+\n",
            "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
            "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
            "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
            "+---+---+-------+----------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, you can enable `spark.sql.repl.eagerEval.enabled` configuration for the eager evaluation of PySpark DataFrame in notebooks such as Jupyter. The number of rows to show can be controlled via `spark.sql.repl.eagerEval.maxNumRows` configuration."
      ],
      "metadata": {
        "id": "v2MLsufIWyy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "id": "_4C0I5bnVJnt",
        "outputId": "61b5ef6b-7c44-420c-faef-c0bbf58dba17"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
            ],
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n",
              "<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n",
              "<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02 12:00:00</td></tr>\n",
              "<tr><td>3</td><td>4.0</td><td>string3</td><td>2000-03-01</td><td>2000-01-03 12:00:00</td></tr>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98VuRZTRVJlD",
        "outputId": "ce2c9f43-0186-4b09-9c0b-c10ccc06ac77"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-------+----------+-------------------+\n",
            "|  a|  b|      c|         d|                  e|\n",
            "+---+---+-------+----------+-------------------+\n",
            "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
            "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
            "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
            "+---+---+-------+----------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(1, vertical=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5bizuqlVJic",
        "outputId": "1271a748-934e-476d-e255-d68814b5076a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-RECORD 0------------------\n",
            " a   | 1                   \n",
            " b   | 2.0                 \n",
            " c   | string1             \n",
            " d   | 2000-01-01          \n",
            " e   | 2000-01-01 12:00:00 \n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMX1n7CzVJf4",
        "outputId": "49a5580b-f33d-45c4-dd78-060227b56397"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a', 'b', 'c', 'd', 'e']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SduTb5XuVJcw",
        "outputId": "08435d1e-50e5-492a-c69e-9f1842555dc1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- a: long (nullable = true)\n",
            " |-- b: double (nullable = true)\n",
            " |-- c: string (nullable = true)\n",
            " |-- d: date (nullable = true)\n",
            " |-- e: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select('a', 'b', 'c').describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqRnN7rkVJaI",
        "outputId": "2c2a9f78-76b3-43ea-e85f-287a690b6c38"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+---+-------+\n",
            "|summary|  a|  b|      c|\n",
            "+-------+---+---+-------+\n",
            "|  count|  3|  3|      3|\n",
            "|   mean|2.0|3.0|   null|\n",
            "| stddev|1.0|1.0|   null|\n",
            "|    min|  1|2.0|string1|\n",
            "|    max|  3|4.0|string3|\n",
            "+-------+---+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`DataFrame.collect()` collects the distributed data to the driver side as the local data in Python. Note that this can throw an out-of-memory error when the dataset is too large to fit in the driver side because it collects all the data from executors to the driver side."
      ],
      "metadata": {
        "id": "5xMQaQmyXyC2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to avoid throwing an out-of-memory exception, use DataFrame.take() or DataFrame.tail().\n",
        "\n"
      ],
      "metadata": {
        "id": "yBMpNv5NX4SH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0M187qPVJYD",
        "outputId": "a2662a1a-039c-40cc-e8f6-cc2f5c285c3f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0))]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark DataFrame also provides the conversion back to a pandas DataFrame to leverage pandas API. Note that toPandas also collects all data into the driver side that can easily cause an out-of-memory-error when the data is too large to fit into the driver side."
      ],
      "metadata": {
        "id": "sIjl1yWmYA0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df.toPandas()"
      ],
      "metadata": {
        "id": "LwpfbV8mVJU6"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selecting and Accessing Data"
      ],
      "metadata": {
        "id": "rKb9-4t5n7K5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3h_aqF5dVJSy",
        "outputId": "83c6a2f7-2c28-4635-d86e-a1cdc47dec47"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Column<'a'>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Column\n",
        "from pyspark.sql.functions import upper"
      ],
      "metadata": {
        "id": "ZXVd63miVJF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(df.c) == type(upper(df.c)) == type(df.c.isNull())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhaW8JS7VJAk",
        "outputId": "ac6148ac-1727-4335-bf4c-b1eb1ec9dec9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These `Column`s can be used to select the columns from a DataFrame. For example, `DataFrame.select()` takes the `Column` instances that returns another DataFrame."
      ],
      "metadata": {
        "id": "F1hXuT3PpgOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(df.c).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ro25PRwVVI98",
        "outputId": "d3d8c48e-af2b-4606-93f3-9d7001fd1d50"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|      c|\n",
            "+-------+\n",
            "|string1|\n",
            "|string2|\n",
            "|string3|\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn('upper_c', upper(df.c)).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yQtQ1GaVI60",
        "outputId": "211a16e1-90cb-4d48-8680-4eeebc10bfe5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-------+----------+-------------------+-------+\n",
            "|  a|  b|      c|         d|                  e|upper_c|\n",
            "+---+---+-------+----------+-------------------+-------+\n",
            "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|STRING1|\n",
            "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|STRING2|\n",
            "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|STRING3|\n",
            "+---+---+-------+----------+-------------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "id": "pncJ82NvVI4N",
        "outputId": "326063a9-3ae1-405c-ebe1-a43563998eac"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
            ],
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n",
              "<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n",
              "<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02 12:00:00</td></tr>\n",
              "<tr><td>3</td><td>4.0</td><td>string3</td><td>2000-03-01</td><td>2000-01-03 12:00:00</td></tr>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df.a == 1).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDNExg8TVI2I",
        "outputId": "c490caa6-e129-4956-fdfa-0323b4cc48b9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-------+----------+-------------------+\n",
            "|  a|  b|      c|         d|                  e|\n",
            "+---+---+-------+----------+-------------------+\n",
            "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
            "+---+---+-------+----------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying a Function\n",
        "\n",
        "Pyspark supports various UDFs and APIs to allow users to execute Python native functions.\n",
        "\n",
        "See also the latest [Pandas UDFs](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html#pandas-udfs-aka-vectorized-udfs) and [Pandas Function APIs](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html#pandas-function-apis). For instance, the example below allows users to directly use the APIs in [a pandas Series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html) within Python native function."
      ],
      "metadata": {
        "id": "Hb-Wir4Ip1YK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "from pyspark.sql.functions import pandas_udf"
      ],
      "metadata": {
        "id": "kTcFWufLVIzi"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@pandas_udf('long')\n",
        "def pandas_plus_one(series: pd.Series) -> pd.Series:\n",
        "    return series + 1"
      ],
      "metadata": {
        "id": "EUxezaB_VIw4"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(pandas_plus_one(df.a)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHqZRF8UVIux",
        "outputId": "e41012c6-6f34-4f99-f0b2-649fb88d4ab3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+\n",
            "|pandas_plus_one(a)|\n",
            "+------------------+\n",
            "|                 2|\n",
            "|                 3|\n",
            "|                 4|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another example is `DataFrame.mapInPandas` which allows users directly use the APIs in a [pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) without any restrictions such as the result length."
      ],
      "metadata": {
        "id": "MV6DdG0aq2y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pandas_filter_func(iterator):\n",
        "    for pandas_df in iterator:\n",
        "        yield pandas_df[pandas_df.a == 1]\n",
        "\n",
        "df.mapInPandas(pandas_filter_func, schema=df.schema).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbGoHo8jVIsN",
        "outputId": "d42d6d71-901c-4586-97ea-62eea5ff6f78"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-------+----------+-------------------+\n",
            "|  a|  b|      c|         d|                  e|\n",
            "+---+---+-------+----------+-------------------+\n",
            "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
            "+---+---+-------+----------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grouping Data\n",
        "\n",
        "PySpark DataFrame also provides a way of handling grouped data by using the common approach, split-apply-combine strategy.\n",
        "It groups the data by a certain condition applies a function to each group and then combines them back to the DataFrame."
      ],
      "metadata": {
        "id": "WYeDvtytrx5N"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2-8uO3CPVIpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0r5kOazsVIm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "37Vn9tMQVIkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cg6bDLc6VIhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D6vYNt-cVIfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FjTh9gmrVIdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8t7sCRSPVIae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EPTaAZ09VIX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MhsdxLbIVIVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XPhOju1WVITJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tzZ4Ry6tVIQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D1c1z1R4VIOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QHVo8BMRVIMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CTWKVjYDVIJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6KZh1e3pVIHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lv770eqbVIFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y4IjACEeVIC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9lSgzZRTVIAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qdXkHkTmVH-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YZ_uHiDnVH7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E4_Zm3PgVH5P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}